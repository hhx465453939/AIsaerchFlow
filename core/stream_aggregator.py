"""
æµå¼å¤šå¹³å°æœç´¢èšåˆå™¨
ä¸“æ³¨äºå®æ—¶ç›‘æ§å’ŒåŠ¨æ€å†…å®¹èšåˆ
"""

import time
import logging
from typing import List, Dict, Any, Optional, Tuple
from dataclasses import dataclass
from datetime import datetime
import threading
import queue
import os
import sys

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

@dataclass
class StreamContent:
    """æµå¼å†…å®¹æ•°æ®ç»“æ„"""
    platform: str
    content: str
    timestamp: str
    is_complete: bool = False
    confidence: float = 1.0

class PlatformStreamMonitor:
    """å•ä¸ªå¹³å°çš„æµå¼ç›‘æ§å™¨"""
    
    def __init__(self, platform_name: str, page_selector: str):
        self.platform_name = platform_name
        self.page_selector = page_selector
        self.content_queue = queue.Queue()
        self.is_monitoring = False
        self.last_content = ""
        
    def monitor_stream(self, page, timeout: int = 60) -> StreamContent:
        """ç›‘æ§é¡µé¢çš„æµå¼è¾“å‡º"""
        self.is_monitoring = True
        start_time = time.time()
        last_update = time.time()
        accumulated_content = ""
        
        logger.info(f"å¼€å§‹ç›‘æ§ {self.platform_name} çš„æµå¼è¾“å‡º")
        
        while self.is_monitoring and (time.time() - start_time) < timeout:
            try:
                # è·å–å½“å‰é¡µé¢å†…å®¹
                current_content = page.inner_text(self.page_selector)
                
                # æ£€æŸ¥æ˜¯å¦æœ‰æ–°å†…å®¹
                if current_content != self.last_content and current_content:
                    accumulated_content = current_content
                    self.last_content = current_content
                    last_update = time.time()
                    
                    logger.debug(f"{self.platform_name} å†…å®¹æ›´æ–°: {len(current_content)} å­—ç¬¦")
                
                # æ£€æŸ¥æ˜¯å¦è¾“å‡ºå®Œæˆï¼ˆ5ç§’æ— æ–°å†…å®¹ï¼‰
                if time.time() - last_update > 5 and accumulated_content:
                    logger.info(f"{self.platform_name} è¾“å‡ºå®Œæˆ")
                    self.is_monitoring = False
                    return StreamContent(
                        platform=self.platform_name,
                        content=accumulated_content,
                        timestamp=datetime.now().isoformat(),
                        is_complete=True
                    )
                
                time.sleep(0.5)  # é¿å…è¿‡åº¦æŸ¥è¯¢
                
            except Exception as e:
                logger.error(f"{self.platform_name} ç›‘æ§å¼‚å¸¸: {e}")
                time.sleep(1)
        
        # è¶…æ—¶æˆ–å¼‚å¸¸ï¼Œè¿”å›å·²æ”¶é›†çš„å†…å®¹
        return StreamContent(
            platform=self.platform_name,
            content=accumulated_content,
            timestamp=datetime.now().isoformat(),
            is_complete=False,
            confidence=0.8 if accumulated_content else 0.0
        )
    
    def stop_monitoring(self):
        """åœæ­¢ç›‘æ§"""
        self.is_monitoring = False

class MultiPlatformStreamAggregator:
    """å¤šå¹³å°æµå¼èšåˆå™¨"""
    
    def __init__(self):
        self.platform_monitors = {}
        self.aggregated_content = []
        self.is_aggregating = False
        
        # å¹³å°é…ç½® - ç®€åŒ–ç‰ˆï¼Œä¸“æ³¨äºæµå¼ç›‘æ§
        self.platform_configs = {
            "DeepSeek": ".c08e6e93",  # æ¶ˆæ¯å†…å®¹é€‰æ‹©å™¨
            "Kimi": ".message-list .message-item:last-child",
            "æ™ºè°±æ¸…è¨€": ".message-area-LmU13Q .markdown-body:last-child",
            # å…¶ä»–å¹³å°å¯ä»¥ç»§ç»­æ·»åŠ 
        }
    
    def start_aggregation(self, platforms: List[str], query: str, 
                         ai_processor_config: Optional[Dict] = None) -> Dict[str, Any]:
        """å¼€å§‹å¤šå¹³å°æµå¼èšåˆ"""
        logger.info(f"å¼€å§‹å¤šå¹³å°æµå¼èšåˆ: {platforms}")
        
        self.is_aggregating = True
        self.aggregated_content = []
        
        # æ”¶é›†æµå¼å†…å®¹ï¼ˆæ¨¡æ‹Ÿç‰ˆæœ¬ï¼Œå®é™…éœ€è¦é›†æˆplaywrightè„šæœ¬ï¼‰
        stream_results = []
        for platform in platforms:
            if platform in self.platform_configs:
                try:
                    logger.info(f"å¯åŠ¨ {platform} æœç´¢...")
                    # è¿™é‡Œæ˜¯æ¨¡æ‹Ÿæ•°æ®ï¼Œå®é™…éœ€è¦è°ƒç”¨ç›¸åº”çš„playwrightè„šæœ¬
                    content = self._run_platform_with_stream(platform, query)
                    if content:
                        stream_results.append(content)
                except Exception as e:
                    logger.error(f"{platform} æ‰§è¡Œå¤±è´¥: {e}")
        
        # ç¬¬äºŒé˜¶æ®µï¼šå®æ—¶èšåˆ
        aggregated_result = self._aggregate_streams(stream_results)
        
        # ç¬¬ä¸‰é˜¶æ®µï¼šAIå¤„ç†ï¼ˆå¦‚æœé…ç½®äº†ï¼‰
        if ai_processor_config:
            final_result = self._process_with_ai(aggregated_result, ai_processor_config)
        else:
            final_result = self._simple_merge(aggregated_result)
        
        self.is_aggregating = False
        
        return {
            "query": query,
            "platforms": platforms,
            "stream_results": [
                {
                    "platform": content.platform,
                    "content": content.content,
                    "timestamp": content.timestamp,
                    "is_complete": content.is_complete,
                    "confidence": content.confidence
                }
                for content in stream_results
            ],
            "aggregated_result": final_result,
            "processing_time": datetime.now().isoformat()
        }
    
    def _run_platform_with_stream(self, platform: str, query: str) -> Optional[StreamContent]:
        """è¿è¡Œå•ä¸ªå¹³å°å¹¶è¿›è¡Œæµå¼ç›‘æ§ï¼ˆæ¨¡æ‹Ÿç‰ˆæœ¬ï¼‰"""
        # æ¨¡æ‹Ÿæœç´¢å†…å®¹
        mock_content = f"""
## {platform} æœç´¢ç»“æœ

å…³äº "{query}" çš„å›ç­”ï¼š

{platform} æ˜¯ä¸€ä¸ªå¼ºå¤§çš„AIåŠ©æ‰‹å¹³å°ï¼Œèƒ½å¤Ÿæä¾›é«˜è´¨é‡çš„å›ç­”ã€‚

### ä¸»è¦ç‰¹ç‚¹ï¼š
1. æ™ºèƒ½å¯¹è¯èƒ½åŠ›
2. å¤šé¢†åŸŸçŸ¥è¯†è¦†ç›–
3. å®æ—¶äº¤äº’ä½“éªŒ
4. å‡†ç¡®çš„ä¿¡æ¯æä¾›

### ä½¿ç”¨å»ºè®®ï¼š
- æä¾›æ¸…æ™°çš„é—®é¢˜æè¿°
- æŒ‡å®šå…·ä½“çš„éœ€æ±‚
- å–„ç”¨ä¸Šä¸‹æ–‡ä¿¡æ¯

è¿™æ˜¯ä¸€ä¸ªæ¨¡æ‹Ÿçš„æœç´¢ç»“æœï¼Œç”¨äºæµ‹è¯•é¡¹ç›®çš„åŸºç¡€åŠŸèƒ½ã€‚
        """.strip()
        
        return StreamContent(
            platform=platform,
            content=mock_content,
            timestamp=datetime.now().isoformat(),
            is_complete=True,
            confidence=0.9
        )
    
    def _aggregate_streams(self, stream_results: List[StreamContent]) -> Dict[str, Any]:
        """èšåˆæµå¼ç»“æœ"""
        logger.info("å¼€å§‹èšåˆæµå¼ç»“æœ")
        
        # ç®€å•èšåˆç­–ç•¥
        high_confidence_results = [
            result for result in stream_results 
            if result.confidence > 0.7 and result.is_complete
        ]
        
        return {
            "total_sources": len(stream_results),
            "high_confidence_sources": len(high_confidence_results),
            "contents": [
                {
                    "platform": result.platform,
                    "content": result.content,
                    "confidence": result.confidence
                }
                for result in high_confidence_results
            ]
        }
    
    def _process_with_ai(self, aggregated_data: Dict[str, Any], 
                        config: Dict[str, Any]) -> Dict[str, Any]:
        """ä½¿ç”¨AIå¤„ç†èšåˆç»“æœ"""
        logger.info("å¼€å§‹AIå¤„ç†èšåˆç»“æœ")
        
        # æ„å»ºæç¤ºè¯
        contents = aggregated_data["contents"]
        content_text = "\n".join([
            f"**{item['platform']}** (å¯ä¿¡åº¦: {item['confidence']:.1f}):\n{item['content']}\n"
            for item in contents
        ])
        
        # åŠ è½½æç¤ºè¯æ¨¡æ¿
        prompt_template = self._load_prompt_template(config.get("prompt_type", "default"))
        
        final_prompt = prompt_template.format(
            query=config.get("original_query", ""),
            content=content_text,
            requirements=config.get("requirements", "è¯·æ•´ç†å¹¶æ€»ç»“ä»¥ä¸Šä¿¡æ¯")
        )
        
        # è°ƒç”¨AIæœåŠ¡ï¼ˆæ¨¡æ‹Ÿï¼‰
        ai_result = self._call_ai_service(final_prompt, config)
        
        return {
            "ai_processed": True,
            "original_aggregation": aggregated_data,
            "ai_result": ai_result,
            "prompt_used": prompt_template,
            "processing_config": config
        }
    
    def _simple_merge(self, aggregated_data: Dict[str, Any]) -> Dict[str, Any]:
        """ç®€å•åˆå¹¶ï¼ˆæ— AIå¤„ç†ï¼‰"""
        contents = aggregated_data["contents"]
        
        merged_content = "# å¤šå¹³å°æœç´¢ç»“æœæ±‡æ€»\n\n"
        for i, item in enumerate(contents, 1):
            merged_content += f"## æ¥æº {i}: {item['platform']}\n"
            merged_content += f"**å¯ä¿¡åº¦**: {item['confidence']:.1f}\n\n"
            merged_content += f"{item['content']}\n\n---\n\n"
        
        return {
            "ai_processed": False,
            "merged_content": merged_content,
            "source_count": len(contents)
        }
    
    def _load_prompt_template(self, prompt_type: str) -> str:
        """åŠ è½½æç¤ºè¯æ¨¡æ¿"""
        templates = {
            "default": """
è¯·åŸºäºä»¥ä¸‹å¤šå¹³å°æœç´¢ç»“æœï¼Œè¿›è¡Œä¿¡æ¯æ•´ç†å’Œäº‹å®æ ¸æŸ¥ï¼š

**åŸå§‹æŸ¥è¯¢**: {query}

**æœç´¢ç»“æœ**:
{content}

**å¤„ç†è¦æ±‚**: {requirements}

è¯·æä¾›ï¼š
1. æ ¸å¿ƒä¿¡æ¯æ€»ç»“
2. å…³é”®è¦ç‚¹æå–
3. ä¿¡æ¯å¯ä¿¡åº¦è¯„ä¼°
4. æ½œåœ¨é£é™©æç¤º
5. å»ºè®®è¡ŒåŠ¨æ–¹æ¡ˆ
""",
            "fact_check": """
è¯·å¯¹ä»¥ä¸‹ä¿¡æ¯è¿›è¡Œä¸“ä¸šçš„äº‹å®æ ¸æŸ¥å’Œåˆ†æï¼š

**æŸ¥è¯¢å†…å®¹**: {query}

**å¾…æ ¸æŸ¥ä¿¡æ¯**:
{content}

è¯·æä¾›ï¼š
1. äº‹å®å‡†ç¡®æ€§éªŒè¯
2. ä¿¡æ¯æ¥æºå¯ä¿¡åº¦åˆ†æ
3. æ½œåœ¨åè§æˆ–è¯¯å¯¼è¯†åˆ«
4. å»ºè®®è¿›ä¸€æ­¥éªŒè¯çš„æ–¹å‘
5. ç»¼åˆå¯ä¿¡åº¦è¯„åˆ†ï¼ˆ1-10åˆ†ï¼‰
""",
            "summary": """
è¯·å°†ä»¥ä¸‹å¤šæºä¿¡æ¯æ•´ç†æˆæ¸…æ™°çš„æ€»ç»“æŠ¥å‘Šï¼š

**ä¸»é¢˜**: {query}

**ä¿¡æ¯æ¥æº**:
{content}

**è¦æ±‚**: {requirements}

è¯·æŒ‰ä»¥ä¸‹ç»“æ„è¾“å‡ºï¼š
- ğŸ“‹ æ‰§è¡Œæ‘˜è¦
- ğŸ” è¯¦ç»†åˆ†æ  
- ğŸ’¡ å…³é”®æ´å¯Ÿ
- âš ï¸ æ³¨æ„äº‹é¡¹
- ğŸ¯ è¡ŒåŠ¨å»ºè®®
"""
        }
        return templates.get(prompt_type, templates["default"])
    
    def _call_ai_service(self, prompt: str, config: Dict[str, Any]) -> Dict[str, Any]:
        """è°ƒç”¨AIæœåŠ¡ï¼ˆæ¨¡æ‹Ÿç‰ˆæœ¬ï¼‰"""
        # æ¨¡æ‹ŸAIå¤„ç†ç»“æœ
        return {
            "content": f"""# AIæ•´ç†ç»“æœ

åŸºäºå¤šå¹³å°æœç´¢ç»“æœçš„ç»¼åˆåˆ†æï¼š

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹æ€»ç»“
é€šè¿‡å¯¹æ¯”å¤šä¸ªAIå¹³å°çš„å›ç­”ï¼Œå‘ç°äº†ä»¥ä¸‹å…³é”®ä¿¡æ¯ï¼š
- å„å¹³å°åœ¨å›ç­”è´¨é‡ä¸Šè¡¨ç°ä¸€è‡´
- ä¸»è¦è§‚ç‚¹å…·æœ‰è¾ƒé«˜çš„ä¸€è‡´æ€§
- ä¿¡æ¯æ¥æºå¯ä¿¡åº¦è‰¯å¥½

## ğŸ” è¯¦ç»†åˆ†æ
{prompt[:200]}...

## ğŸ’¡ å…³é”®æ´å¯Ÿ
- AIå¹³å°èšåˆæœç´¢èƒ½å¤Ÿæä¾›æ›´å…¨é¢çš„ä¿¡æ¯
- å¤šæºéªŒè¯æé«˜äº†ä¿¡æ¯å¯ä¿¡åº¦
- æµå¼ç›‘æ§ç¡®ä¿äº†å†…å®¹çš„å®Œæ•´æ€§

## âš ï¸ æ³¨æ„äº‹é¡¹
- å»ºè®®å¯¹å…³é”®ä¿¡æ¯è¿›è¡Œè¿›ä¸€æ­¥éªŒè¯
- æ³¨æ„ä¸åŒå¹³å°å¯èƒ½å­˜åœ¨çš„åè§
- è€ƒè™‘ä¿¡æ¯çš„æ—¶æ•ˆæ€§

## ğŸ¯ è¡ŒåŠ¨å»ºè®®
1. æ ¹æ®èšåˆç»“æœåˆ¶å®šè¡ŒåŠ¨è®¡åˆ’
2. æŒç»­ç›‘æ§ç›¸å…³ä¿¡æ¯å˜åŒ–
3. å¿…è¦æ—¶è¿›è¡Œæ·±åº¦è°ƒç ”

*æœ¬ç»“æœç”±AIè‡ªåŠ¨ç”Ÿæˆï¼Œä»…ä¾›å‚è€ƒ*
""",
            "model": config.get("model", "simulation"),
            "tokens_used": len(prompt) // 4,  # æ¨¡æ‹Ÿtokenä½¿ç”¨é‡
            "processing_time": 2.5
        }
    
    def stop_aggregation(self):
        """åœæ­¢èšåˆ"""
        self.is_aggregating = False
        for monitor in self.platform_monitors.values():
            monitor.stop_monitoring()

# å¯¼å‡ºä¸»è¦æ¥å£
__all__ = ["MultiPlatformStreamAggregator", "StreamContent", "PlatformStreamMonitor"]

# æµ‹è¯•ä»£ç 
if __name__ == "__main__":
    # ç®€å•æµ‹è¯•
    aggregator = MultiPlatformStreamAggregator()
    result = aggregator.start_aggregation(
        platforms=["DeepSeek", "Kimi"],
        query="ä»€ä¹ˆæ˜¯äººå·¥æ™ºèƒ½ï¼Ÿ"
    )
    
    print("=== æµ‹è¯•ç»“æœ ===")
    print(f"æŸ¥è¯¢: {result['query']}")
    print(f"å¹³å°: {result['platforms']}")
    print(f"ç»“æœæ•°: {len(result['stream_results'])}")
    print("\nèšåˆå†…å®¹:")
    print(result['aggregated_result']['merged_content']) 