import streamlit as st
import requests
import os
import json
from typing import Dict, Any, Optional

API_URL = "http://localhost:8000"
PLATFORMS = [
    "DeepSeek", "Kimi", "æ™ºè°±æ¸…è¨€", "ç§˜å¡”æœç´¢", "è±†åŒ…", "Qwen", "MiniMax", "ChatGPT", "Gemini"
]
PLATFORM_URLS = {
    "DeepSeek": "https://chat.deepseek.com/",
    "Kimi": "https://kimi.moonshot.cn/",
    "æ™ºè°±æ¸…è¨€": "https://chatglm.cn/main/alltoolsdetail?lang=zh",
    "ç§˜å¡”æœç´¢": "https://metaso.cn/",
    "è±†åŒ…": "https://www.doubao.com/chat/",
    "Qwen": "https://chat.qwen.ai/",
    "MiniMax": "https://chat.minimaxi.com/",
    "ChatGPT": "https://chatgpt.com/",
    "Gemini": "https://gemini.google.com/app?hl=zh-cn",
}

st.set_page_config(page_title="å¤šå¹³å°AIæœç´¢ v2.0", layout="wide")
st.title("ğŸ¤– å¤šå¹³å°AIæœç´¢èšåˆ v2.0")
st.markdown("*é›†æˆAIæ•´åˆã€äº‹å®æ ¸æŸ¥ã€è¯­ä¹‰å»é‡çš„æ™ºèƒ½æœç´¢å¹³å°*")

# åˆå§‹åŒ–ä¼šè¯çŠ¶æ€
if "accounts" not in st.session_state:
    st.session_state["accounts"] = {}
if "login_status" not in st.session_state:
    st.session_state["login_status"] = {}
if "llm_config" not in st.session_state:
    st.session_state["llm_config"] = {
        "base_url": "https://api.openai.com/v1",
        "api_key": "",
        "model": "gpt-3.5-turbo",
        "temperature": 0.3,
        "max_tokens": 4000
    }
if "agg_prompt" not in st.session_state:
    # é»˜è®¤åŠ è½½æ–‡æ¡£æ¶æ„å¸ˆæç¤ºè¯
    try:
        with open("prompt/# Roleï¼šæ–‡æ¡£æ¶æ„å¸ˆ.md", encoding="utf-8") as f:
            st.session_state["agg_prompt"] = f.read()
    except Exception:
        st.session_state["agg_prompt"] = "è¯·å¡«å†™èšåˆæç¤ºè¯"
if "browser_choice" not in st.session_state:
    st.session_state["browser_choice"] = "Edge"
if "user_data_dir" not in st.session_state:
    st.session_state["user_data_dir"] = ""
if "ai_features" not in st.session_state:
    st.session_state["ai_features"] = {
        "use_ai_integration": True,
        "enable_fact_check": True,
        "semantic_deduplication": True
    }

# ä¸»ç•Œé¢
col1, col2 = st.columns([2, 1])

with col1:
    st.subheader("ğŸ” æœç´¢é…ç½®")
    user_input = st.text_input("è¯·è¾“å…¥ä½ çš„æœç´¢å†…å®¹ï¼š", key="search_input")
    platforms = st.multiselect("é€‰æ‹©å¹³å°ï¼š", PLATFORMS, default=["DeepSeek", "Kimi"], key="platforms")
    
    # AIåŠŸèƒ½æ§åˆ¶
    st.subheader("ğŸ§  AIå¢å¼ºåŠŸèƒ½")
    col_ai1, col_ai2, col_ai3 = st.columns(3)
    
    with col_ai1:
        use_ai_integration = st.checkbox(
            "å¯ç”¨AIæ•´åˆ", 
            value=st.session_state["ai_features"]["use_ai_integration"],
            help="ä½¿ç”¨AIæ¨¡å‹è¿›è¡Œæ™ºèƒ½å»é‡å’Œä¿¡æ¯æ•´åˆ"
        )
        st.session_state["ai_features"]["use_ai_integration"] = use_ai_integration
    
    with col_ai2:
        enable_fact_check = st.checkbox(
            "å¯ç”¨äº‹å®æ ¸æŸ¥", 
            value=st.session_state["ai_features"]["enable_fact_check"],
            help="å¯¹æœç´¢ç»“æœè¿›è¡Œè”ç½‘äº‹å®æ ¸æŸ¥"
        )
        st.session_state["ai_features"]["enable_fact_check"] = enable_fact_check
    
    with col_ai3:
        semantic_deduplication = st.checkbox(
            "è¯­ä¹‰å»é‡", 
            value=st.session_state["ai_features"]["semantic_deduplication"],
            help="åŸºäºè¯­ä¹‰ç›¸ä¼¼åº¦è¿›è¡Œæ™ºèƒ½å»é‡"
        )
        st.session_state["ai_features"]["semantic_deduplication"] = semantic_deduplication

with col2:
    st.subheader("âš™ï¸ çŠ¶æ€ç›‘æ§")
    
    # æ˜¾ç¤ºAPIå¥åº·çŠ¶æ€
    try:
        health_resp = requests.get(f"{API_URL}/health", timeout=5)
        if health_resp.ok:
            health_data = health_resp.json()
            st.success(f"âœ… APIçŠ¶æ€: {health_data['status']}")
            st.info(f"ç‰ˆæœ¬: {health_data['version']}")
        else:
            st.error("âŒ APIè¿æ¥å¤±è´¥")
    except:
        st.error("âŒ APIä¸å¯ç”¨")
    
    # LLMé…ç½®çŠ¶æ€
    if st.session_state["llm_config"]["api_key"]:
        st.success("âœ… LLMå·²é…ç½®")
    else:
        st.warning("âš ï¸ æœªé…ç½®LLM")

# ä¾§è¾¹æ ï¼šè¯¦ç»†é…ç½®
st.sidebar.header("ğŸ”§ è¯¦ç»†é…ç½®")

# LLMé…ç½®
st.sidebar.subheader("ğŸ¤– LLMé…ç½®")
st.sidebar.markdown("*AIæ•´åˆå’Œäº‹å®æ ¸æŸ¥éœ€è¦LLMæ”¯æŒ*")

llm_base_url = st.sidebar.text_input(
    "LLM Base URL", 
    value=st.session_state["llm_config"]["base_url"],
    help="å¦‚: https://api.openai.com/v1 æˆ– https://api.siliconflow.cn/v1"
)
llm_api_key = st.sidebar.text_input(
    "API Key", 
    type="password",
    value=st.session_state["llm_config"]["api_key"],
    help="LLMæœåŠ¡çš„APIå¯†é’¥"
)
llm_model = st.sidebar.text_input(
    "æ¨¡å‹åç§°", 
    value=st.session_state["llm_config"]["model"],
    help="å¦‚: gpt-3.5-turbo, qwen-vl-plus"
)

col_temp, col_tokens = st.sidebar.columns(2)
with col_temp:
    llm_temperature = st.number_input(
        "Temperature", 
        min_value=0.0, max_value=2.0, 
        value=st.session_state["llm_config"]["temperature"],
        step=0.1
    )
with col_tokens:
    llm_max_tokens = st.number_input(
        "Max Tokens", 
        min_value=100, max_value=8000,
        value=st.session_state["llm_config"]["max_tokens"],
        step=100
    )

# æ›´æ–°LLMé…ç½®
st.session_state["llm_config"].update({
    "base_url": llm_base_url,
    "api_key": llm_api_key,
    "model": llm_model,
    "temperature": llm_temperature,
    "max_tokens": llm_max_tokens
})

# éªŒè¯LLMé…ç½®
if st.sidebar.button("ğŸ” éªŒè¯LLMé…ç½®"):
    if llm_api_key and llm_base_url:
        with st.sidebar.spinner("éªŒè¯ä¸­..."):
            try:
                resp = requests.post(f"{API_URL}/validate-llm-config", json={
                    "base_url": llm_base_url,
                    "api_key": llm_api_key,
                    "model": llm_model,
                    "temperature": llm_temperature,
                    "max_tokens": llm_max_tokens
                })
                if resp.ok:
                    result = resp.json()
                    if result["valid"]:
                        st.sidebar.success(f"âœ… {result['message']}")
                    else:
                        st.sidebar.error(f"âŒ {result['message']}")
                else:
                    st.sidebar.error("âŒ éªŒè¯å¤±è´¥")
            except Exception as e:
                st.sidebar.error(f"âŒ éªŒè¯å¼‚å¸¸: {e}")
    else:
        st.sidebar.warning("âš ï¸ è¯·å¡«å†™å®Œæ•´çš„LLMé…ç½®")

# æµè§ˆå™¨é…ç½®
st.sidebar.subheader("ğŸŒ æµè§ˆå™¨é…ç½®")
browser_choice = st.sidebar.selectbox(
    "é€‰æ‹©è‡ªåŠ¨åŒ–æµè§ˆå™¨", 
    ["Edge", "Chrome", "è‡ªå®šä¹‰è·¯å¾„"], 
    key="browser_choice"
)
if browser_choice == "è‡ªå®šä¹‰è·¯å¾„":
    user_data_dir = st.sidebar.text_input("æµè§ˆå™¨ç”¨æˆ·æ•°æ®ç›®å½•", key="user_data_dir")
else:
    user_data_dir = ""

# è´¦å·ç®¡ç†
st.sidebar.subheader("ğŸ‘¤ è´¦å·ç®¡ç†")
if st.sidebar.button("ğŸ”„ ä¸€é”®æ£€æµ‹/å¯¼å…¥ç™»å½•çŠ¶æ€"):
    with st.sidebar.spinner("æ£€æµ‹ä¸­..."):
        # è¿™é‡Œå¯ä»¥æ·»åŠ è‡ªåŠ¨å¯¼å…¥é€»è¾‘
        st.sidebar.info("è¯·ä½¿ç”¨ import_edge_cookie.py å¯¼å…¥Cookie")

# æ˜¾ç¤ºç™»å½•çŠ¶æ€
for platform in platforms:
    status = st.session_state["login_status"].get(platform, "æœªæ£€æµ‹")
    if status == "å·²ç™»å½•":
        st.sidebar.success(f"âœ… {platform}")
    else:
        st.sidebar.warning(f"âš ï¸ {platform}: {status}")
        if st.sidebar.button(f"å‰å¾€ {platform} ç™»å½•", key=f"goto_{platform}"):
            st.sidebar.markdown(f"[ç‚¹å‡»å‰å¾€]({PLATFORM_URLS[platform]})")

# æœç´¢æ‰§è¡Œ
st.markdown("---")
col_search1, col_search2 = st.columns(2)

with col_search1:
    if st.button("ğŸš€ AIå¢å¼ºæœç´¢", type="primary", use_container_width=True):
        if not user_input:
            st.error("è¯·è¾“å…¥æœç´¢å†…å®¹")
        elif not platforms:
            st.error("è¯·é€‰æ‹©è‡³å°‘ä¸€ä¸ªå¹³å°")
        else:
            # æ£€æŸ¥AIåŠŸèƒ½é…ç½®
            if use_ai_integration and not llm_api_key:
                st.warning("å¯ç”¨äº†AIæ•´åˆä½†æœªé…ç½®LLMï¼Œå°†ä½¿ç”¨ä¼ ç»Ÿæœç´¢")
                use_ai_integration = False
            
            with st.spinner("ğŸ”„ AIå¢å¼ºæœç´¢ä¸­..."):
                try:
                    # å‡†å¤‡è¯·æ±‚æ•°æ®
                    search_data = {
                        "user_input": user_input,
                        "platforms": platforms,
                        "use_ai_integration": use_ai_integration,
                        "enable_fact_check": enable_fact_check and use_ai_integration,
                        "browser_config": {
                            "browser": browser_choice,
                            "user_data_dir": user_data_dir
                        }
                    }
                    
                    # æ·»åŠ LLMé…ç½®
                    if use_ai_integration and llm_api_key:
                        search_data["llm_config"] = st.session_state["llm_config"]
                    
                    # å‘é€è¯·æ±‚
                    resp = requests.post(f"{API_URL}/ai-search", json=search_data, timeout=300)
                    
                    if resp.ok:
                        result = resp.json()
                        if result["success"]:
                            data = result["data"]
                            
                            # æ˜¾ç¤ºå¤„ç†ä¿¡æ¯
                            st.success("ğŸ‰ AIå¢å¼ºæœç´¢å®Œæˆï¼")
                            
                            # æ˜¾ç¤ºå¤„ç†ç»Ÿè®¡
                            processing_info = data["processing_info"]
                            col_stat1, col_stat2, col_stat3, col_stat4 = st.columns(4)
                            
                            with col_stat1:
                                st.metric("åŸå§‹ç»“æœ", processing_info["total_results_count"])
                            with col_stat2:
                                st.metric("æœ‰æ•ˆç»“æœ", processing_info["valid_results_count"])
                            with col_stat3:
                                st.metric("AIæ•´åˆ", "âœ…" if processing_info["ai_integration_enabled"] else "âŒ")
                            with col_stat4:
                                st.metric("äº‹å®æ ¸æŸ¥", "âœ…" if processing_info["fact_check_enabled"] else "âŒ")
                            
                            # æ˜¾ç¤ºæ•´åˆç»“æœ
                            st.subheader("ğŸ“„ æ•´åˆæ–‡æ¡£")
                            integration_result = data["integration_result"]
                            integrated_content = integration_result["integrated_document"]["integrated_content"]
                            st.markdown(integrated_content)
                            
                            # æ˜¾ç¤ºå¤„ç†è¯¦æƒ…
                            with st.expander("ğŸ” å¤„ç†è¯¦æƒ…"):
                                for i, content in enumerate(integration_result["processed_contents"]):
                                    st.write(f"**æ¥æº {i+1}: {content['platform']}**")
                                    st.write(f"å¯ä¿¡åº¦: {content['confidence_score']:.2f}")
                                    st.write(f"äº‹å®æ ¸æŸ¥: {'âœ…' if content['fact_checked'] else 'âŒ'}")
                                    st.text_area(f"å†…å®¹ {i+1}", content['content'], height=100, key=f"content_{i}")
                                    st.markdown("---")
                            
                            # æ˜¾ç¤ºåŸå§‹ç»“æœ
                            with st.expander("ğŸ“Š åŸå§‹ç»“æœ"):
                                for platform, raw_result in data["raw_results"]:
                                    st.write(f"**{platform}:**")
                                    st.text(raw_result)
                                    st.markdown("---")
                        else:
                            st.error("æœç´¢å¤±è´¥")
                    else:
                        st.error(f"è¯·æ±‚å¤±è´¥: {resp.status_code}")
                        
                except Exception as e:
                    st.error(f"æœç´¢å¼‚å¸¸: {e}")

with col_search2:
    if st.button("ğŸ” ä¼ ç»Ÿæœç´¢", use_container_width=True):
        if not user_input:
            st.error("è¯·è¾“å…¥æœç´¢å†…å®¹")
        elif not platforms:
            st.error("è¯·é€‰æ‹©è‡³å°‘ä¸€ä¸ªå¹³å°")
        else:
            with st.spinner("ğŸ”„ ä¼ ç»Ÿæœç´¢ä¸­..."):
                try:
                    resp = requests.post(f"{API_URL}/search", json={
                        "user_input": user_input,
                        "platforms": platforms
                    }, timeout=300)
                    
                    if resp.ok:
                        data = resp.json()
                        st.success("âœ… ä¼ ç»Ÿæœç´¢å®Œæˆ")
                        
                        st.subheader("ğŸ“„ å»é‡åç»“æœ")
                        for i, item in enumerate(data["results"]):
                            st.text_area(f"ç»“æœ {i+1}", item, height=100, key=f"trad_{i}")
                        
                        with st.expander("ğŸ“Š åŸå§‹ç»“æœ"):
                            for platform, result in data["raw"]:
                                st.write(f"**{platform}:**")
                                st.text(result)
                                st.markdown("---")
                    else:
                        st.error("æœç´¢å¤±è´¥")
                        
                except Exception as e:
                    st.error(f"æœç´¢å¼‚å¸¸: {e}")

# åº•éƒ¨ä¿¡æ¯
st.markdown("---")
st.markdown("### ğŸ’¡ ä½¿ç”¨æç¤º")
st.markdown("""
- **AIå¢å¼ºæœç´¢**: ä½¿ç”¨AIæ¨¡å‹è¿›è¡Œæ™ºèƒ½å»é‡ã€ä¿¡æ¯æ•´åˆå’Œäº‹å®æ ¸æŸ¥
- **ä¼ ç»Ÿæœç´¢**: ä½¿ç”¨ç®€å•å»é‡ï¼Œæ— éœ€LLMé…ç½®
- **é…ç½®LLM**: æ”¯æŒOpenAIã€SiliconFlowç­‰å…¼å®¹OpenAI APIçš„æœåŠ¡
- **äº‹å®æ ¸æŸ¥**: å¯¹æœç´¢ç»“æœè¿›è¡Œå¯ä¿¡åº¦è¯„ä¼°å’Œäº‹å®éªŒè¯
- **è¯­ä¹‰å»é‡**: åŸºäºå†…å®¹è¯­ä¹‰è€Œéæ–‡æœ¬ç›¸ä¼¼åº¦è¿›è¡Œå»é‡
""")

# çŠ¶æ€æ 
if st.session_state["ai_features"]["use_ai_integration"]:
    if st.session_state["llm_config"]["api_key"]:
        st.success("ğŸ¤– AIå¢å¼ºæ¨¡å¼å·²å¯ç”¨")
    else:
        st.warning("âš ï¸ AIå¢å¼ºæ¨¡å¼éœ€è¦é…ç½®LLM")
else:
    st.info("â„¹ï¸ å½“å‰ä½¿ç”¨ä¼ ç»Ÿæ¨¡å¼") 